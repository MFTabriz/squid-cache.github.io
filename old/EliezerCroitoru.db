<?xml version="1.0" encoding="utf-8"?><!DOCTYPE article  PUBLIC '-//OASIS//DTD DocBook XML V4.4//EN'  'http://www.docbook.org/xml/4.4/docbookx.dtd'><article><articleinfo><title>EliezerCroitoru</title><revhistory><revision><revnumber>11</revnumber><date>2020-05-22 13:43:25</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>10</revnumber><date>2019-02-23 22:08:53</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>9</revnumber><date>2017-06-21 00:11:34</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>8</revnumber><date>2017-06-21 00:08:53</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>7</revnumber><date>2015-11-25 00:27:55</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>6</revnumber><date>2015-02-24 00:46:41</date><authorinitials>Eliezer Croitoru</authorinitials><revremark>Fixed a typo</revremark></revision><revision><revnumber>5</revnumber><date>2015-02-21 17:39:09</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>4</revnumber><date>2013-11-03 05:58:38</date><authorinitials>AmosJeffries</authorinitials><revremark>spaces not permitted in wikilinks</revremark></revision><revision><revnumber>3</revnumber><date>2012-09-25 05:28:34</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>2</revnumber><date>2012-09-25 05:26:45</date><authorinitials>Eliezer Croitoru</authorinitials></revision><revision><revnumber>1</revnumber><date>2012-09-25 05:25:22</date><authorinitials>Eliezer Croitoru</authorinitials></revision></revhistory></articleinfo><section><title>Eliezer Croitoru</title><para>Email: <code>&lt;ngtech1ltd@gmail.com&gt;</code> </para></section><section><title>2020_05</title><para>OK, So restarting the <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/NgTech#">NgTech</ulink> LTD service. After working on couple projects some time now I have published and offloaded the RPM and binary packages build of Squid-Cahce. Can be seen at: <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/BuildFarm/DockerPackaging#">BuildFarm/DockerPackaging</ulink> </para><para>For now we are downsizing the workload and I am changing the availability of my services in general. I will have Desktop/Office hours so you are welcome to try and contact me directly by any of my contact details of the ngtech.co.il domain. </para><section><title>YouTube Granular videos filtering</title><para>My current project is <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/YouTube#">YouTube</ulink> Granular videos filtering. A draft can be seen at: <ulink url="https://github.com/elico/yt-classification-service-example"/> </para><para>I hope that this idea will be integrated in the wide market. </para></section></section><section><title>2019_02</title><!--rule (<hr>) is not applicable to DocBook--><para> Licensing update about the code I post in the wiki. The code I post in the wiki if not presented a specific License is 3-Clause BSD as in: </para><itemizedlist><listitem><para><ulink url="http://ngtech.co.il/license/">NgTech LTD BSD License</ulink> </para></listitem><listitem><para><ulink url="https://opensource.org/licenses/BSD-3-Clause">The 3-Clause BSD License template</ulink> </para></listitem></itemizedlist><section><title>WCCP monitoring alternative</title><para>I have been working on replacing the logic of WCCP to the favor of a TCP based service that can be monitored from Mikrotik Routers and other Linux base systems such as VyOS, EdgeOS xWRT etc.. The code for now is written in Golang due to it being simple at <ulink url="https://gogs.ngtech.co.il/NgTech-LTD/check-systemd-squid">check-systemd-squid</ulink>. The concept of the monitoring is to use two or three methods </para><itemizedlist><listitem><para>availability of the cache manager info page </para></listitem><listitem><para>availability of a specific web host target such as google, cloudflare or an ISP internal web service </para></listitem><listitem><para>the continues state of a tcp connection to the proxy like in BGP, ...as long as the connection is open to keepalive packets the proxy software is running </para></listitem></itemizedlist><para>I have a complex lab setup with every major OS: </para><itemizedlist><listitem><para>Windows Desktop+Server(2k12,2k16,2k19,7,8.1,10) </para></listitem><listitem><para>Linux Desktop+Server+Router(CentOS,Ubunut,Debian,Alpine,Arch..) </para></listitem><listitem><para>BSD(Free.Open.Nano,BSDRP) </para></listitem><listitem><para>Mikrotik RouterOS </para></listitem><listitem><para>JunOS SRX and MX </para></listitem><listitem><para>VyOS </para></listitem><listitem><para>EdgeOS </para></listitem><listitem><para><emphasis><emphasis role="strong">no</emphasis></emphasis> CISCO </para></listitem></itemizedlist><para>Most of them do not work with WCCP but relay on something like PBR or FBF or another way to pass the traffic towards squid or Intercept the traffic on them. Each and every one of them needs a specific set of configurations but I support Linux and MT for now, the others are just there to understand the market. </para></section><section><title>StoreID YouTube caching V 2019</title><para>I worked on another way to cache <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/YouTube#">YouTube</ulink> videos for Desktop and integrated it both locally and remotely in couple places around the world that has a SAT(WIFI dish) connection in jungles and mountains  and it seems to work as expected (at-least 30% hit rate). </para><para>I will try to add it into the wiki while it's not 100% opensource but will have enough references for these who want to implement something similar. </para><para>Requires: </para><itemizedlist><listitem><para>ICAP Service(reqmod and respomod) </para></listitem><listitem><para>redis-server(optional) </para></listitem><listitem><para>StoreID helper </para></listitem></itemizedlist></section></section><section><title>2017_06</title><!--rule (<hr>) is not applicable to DocBook--><para>I am working on an alternative for wccp implementation. This alternative will be composed from: </para><itemizedlist><listitem><para>A Linux master(s) control node(s) </para></listitem><listitem><para>A set of Linux squids(without disk caching) </para></listitem><listitem><para>A redundant shared storage(nfs or glusterfs) </para></listitem><listitem><para>A set of proxy scripts that will update the state of the node on the shared storage </para></listitem><listitem><para>A Controller service or a set of scripts that will monitor the proxies states using the shared storage </para></listitem><listitem><para>A set of routers PBR\FBF\other control scripts that will be used to control and balance the traffic based on the shared state of the proxies </para></listitem><listitem><para>A set of Ansible playbooks and scripts that will help to deploy a whole setup from the grounds up </para></listitem><listitem><para>SSL-BUMP auto integration\deployment scripts </para></listitem></itemizedlist></section><section><title>2015_04</title><!--rule (<hr>) is not applicable to DocBook--><para> I have released <ulink url="http://www1.ngtech.co.il/wpe/?page_id=135">SquidBlocker</ulink> which can be an alternative to <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/SquidGuard#">SquidGuard</ulink>. If you are already here take a peek at <ulink url="http://www1.ngtech.co.il/squidblocker/">SquidBlocker page</ulink> just to understand a bit more about the different algorithms and ideas.  </para></section><section><title>2015</title><!--rule (<hr>) is not applicable to DocBook--><para>Now After a very long time that my work results <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/Features/StoreID#">StoreID</ulink> and it has been tested of a very long time in production systems and considers Stable.</para><para> I am recommending for who ever reads this page to also take a peek at <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/ConfigExamples/DynamicContent/Coordinator#">Caching Dynamic Content using Adaptation</ulink>.</para><para>This is also the place to say thanks for all the great guidance from <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/AmosJeffries#">Amos Jeffris</ulink>, <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/AlexRousskov#">Alex Rousskov</ulink> the <ulink url="http://www.squid-cache.org/Support/mailing-lists.html#squid-users">squid users community</ulink> and all these &quot;people&quot; who helped and helps me everyday to continue and do my daily routines which makes me happier and thank god every moment. </para></section><section><title>Old times</title><!--rule (<hr>) is not applicable to DocBook--><para>Wrote many helpers for squid such as: &quot;<ulink url="https://github.com/elico/squid-helpers/tree/master/squid_helpers/proxy_hb_check">Heart Beat</ulink>&quot;, &quot;<ulink url="https://wiki.squid-cache.org/EliezerCroitoru/ConfigExamples/DynamicContent/Coordinator#">Caching Dynamic Content using Adaptation</ulink>&quot;, &quot;store url&quot;, &quot;<ulink url="https://github.com/elico/echelon">Echeclon ICAP server</ulink>&quot;,&quot;DNSBL External_acl&quot;, &quot;DNSBL server&quot;. </para><para>I also wrote an external_acl helper framework in ruby for many purposes which support concurrency. </para><para>Currently working on porting Store_url_rewrite from <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/Squid-2.7#">Squid-2.7</ulink> to <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/Squid-3.3#">Squid-3.3</ulink>. </para><para>The plan is to add a &quot;fake store url rewrite&quot; (which was done) and then find the way to internally implement the Store_url_rewrite. </para><para>After reading literally thousands lines of code I'm still optimistic about the next steps. </para><!--rule (<hr>) is not applicable to DocBook--><para> <ulink url="https://wiki.squid-cache.org/EliezerCroitoru/CategoryHomepage#">CategoryHomepage</ulink> </para></section></article>