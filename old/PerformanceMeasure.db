<?xml version="1.0" encoding="utf-8"?><!DOCTYPE article  PUBLIC '-//OASIS//DTD DocBook XML V4.4//EN'  'http://www.docbook.org/xml/4.4/docbookx.dtd'><article><articleinfo><title>PerformanceMeasure</title><revhistory><revision><revnumber>2</revnumber><date>2009-07-02 17:10:49</date><authorinitials>AlexRousskov</authorinitials><revremark>#included discussion (is there a better way to do that?)</revremark></revision><revision><revnumber>1</revnumber><date>2009-05-27 02:19:06</date><authorinitials>AmosJeffries</authorinitials><revremark>brain dump.</revremark></revision></revhistory></articleinfo><section><title>Performance Measurement Bundle</title><itemizedlist><listitem><para><emphasis role="strong">Goal</emphasis>: To provide a bundle of scripts and data which can generate and run a range of traffic throughput tests on Squid. </para></listitem><listitem><para><emphasis role="strong">Status</emphasis>: Concept Design. Not started yet. </para></listitem><listitem><para><emphasis role="strong">ETA</emphasis>: unknown. </para></listitem><listitem><para><emphasis role="strong">Developer</emphasis>: <ulink url="https://wiki.squid-cache.org/PerformanceMeasure/AmosJeffries#">AmosJeffries</ulink> </para></listitem></itemizedlist></section><section><title>Details</title><itemizedlist><listitem override="none"><para><inlinemediaobject><imageobject><imagedata depth="15" fileref="https://wiki.squid-cache.org/wiki/squidtheme/img/alert.png" width="15"/></imageobject><textobject><phrase>/!\</phrase></textobject></inlinemediaobject> This is all a draft brain-dump for now. It has yet to be discussed and checked by anyone. </para></listitem></itemizedlist><para>So just off the top of my head. What we need is: </para><itemizedlist><listitem><para>a simulation of traffic ranging from a few hundred MB up to TB. </para></listitem><listitem><para>something to run this through Squid </para></listitem><listitem><para>something to measure the various things we gather benchmarks on </para></listitem></itemizedlist><para>The simulation cannot be completely random. It needs to be fixed in request/reply sizes and header complexity to give some realistic measure of Squid with live traffic. </para><para>Some things can be randomized: </para><orderedlist numeration="arabic"><listitem><para>Body data. We only require body data of specific sizes. This means we need to know real sizes when creating the benchmark set, but can generate new bodies randomly during benchmark suite setup. This saves a lot of transfer bandwidth and protects against privacy/copyright/security issues at the same time. </para></listitem><listitem><para>Header URL lengths. We do need to keep URL length as realistic as possible. But by its nature we are going to be forced to make URLs into a format our benchmarking server can use to supply the proper reply. They can be padded UP with garbage info to whatever length is needed. This will purge any of the original tracking information and protects against privacy/copyright/security issues. Even though some URL will be larger than their original real version. </para></listitem><listitem><para>Cookie etc. These can be replaced with somewhat random information. Only the header format needs to be kept parsable. </para></listitem></orderedlist><section><title>The Dataset</title><para>For starters I'm thinking we need to use live data. This means we need to locate an ISP able to provide at least one TB of sequential HTTP requests+replies in full to be turned into a benchmarking dataset. </para><para>The data needs to be complete at point of capture, but our generation process can perform the obfuscation methods mentioned above before its published anywhere. If there are any privacy issues, the following can most likely all be done before any information even leaves the ISP. </para><para>When processed the dataset should contain: </para><itemizedlist><listitem><para>a script or file listing the client requests in order. </para></listitem><listitem><para>a script listing the reply objects and size. Headers would need to be packaged with the benchmark bundle, bodies can be generated on setup from this file. </para></listitem><listitem><para>a directory heirarchy of request and reply headers. </para></listitem></itemizedlist></section><section><title>Scripts</title><section><title>Setup</title><para>One to setup the benchmark environment, generate dataset objects etc. This should be pretty simple once the dataset and actual benchmarking scripts are defined. </para></section><section><title>Measurement</title><para>We have a lot of measures already: </para><itemizedlist><listitem><para>req/sec </para></listitem><listitem><para>bytes/sec </para></listitem><listitem><para>total running time for various sizes of dataset </para></listitem><listitem><para>time from start to first server object </para></listitem><listitem><para>time to shutdown after dataset </para></listitem><listitem><para>various measures of per-object timing (variant on object size). </para></listitem><listitem><para>others?? </para></listitem></itemizedlist><para>Most of these are available already from Squid cachemgr reports. The addition of a proposed benchmarking page will greatly enhance the results accuracy. But legacy versions of Squid need to be accommodated for comparison. </para></section><section><title>Client / Server Emulators</title><para>Obviously a client process and server process.  I'm kind of assuming there is software already out there somewhere that can do this fast enough to outperform Squid even after any improvements we make. </para><para>Suggestions please? </para></section></section><section><title>Discussion</title><para><inlinemediaobject><imageobject><imagedata depth="15" fileref="https://wiki.squid-cache.org/wiki/squidtheme/img/alert.png" width="15"/></imageobject><textobject><phrase>/!\</phrase></textobject></inlinemediaobject> To answer, use the &quot;Discussion&quot; link in the main menu </para><para>See <ulink url="https://wiki.squid-cache.org/PerformanceMeasure/PerformanceMeasure#">Discussed Page</ulink> </para><!--rule (<hr>) is not applicable to DocBook--><para> <anchor id="C1"/> In my biased opinion, <ulink url="http://www.web-polygraph.org/">Web Polygraph</ulink> should be used for lab performance work. It already covers your wish list pretty well, has many essential features that are not listed on your page yet, and can be enhanced to do more if needed. Polygraph has been designed to test caching proxies performance and comes with years of performance testing experience. </para><para>If you recall, I have been trying to find time to start running regular Squid performance tests and publish results for more than two years now. I gave up recently and hired somebody who should be able to do this for the Squid Project. I expect to post first results by August 2009. </para><para>I would not recommend using live data for the bulk of the performance testing work. To run meaningful series of tests, you almost always have to <emphasis>tune</emphasis> the workload to match the test purpose. Doing that with live data is sometimes technically possible, but is a lot more complex and does not buy you much in terms of realism. Real data also immediately causes personal and commercial privacy headaches. Parameterizing synthetic workloads with ISP-provided stats is a good idea though. </para><para>You should probably mention a misnamed <ulink url="https://wiki.squid-cache.org/PerformanceMeasure/KnowledgeBase/Benchmarks#">Benchmarks</ulink> page that collects live Squid performance anecdotes. </para><para>-- <ulink url="https://wiki.squid-cache.org/PerformanceMeasure/AlexRousskov#">AlexRousskov</ulink> <!--The macro DateTime caused an error and should be blacklisted. It returned the data '2009-07-02 17:06:34' which caused the docbook-formatter to choke. Please file a bug.--> </para><!--rule (<hr>) is not applicable to DocBook--><para> <ulink url="https://wiki.squid-cache.org/PerformanceMeasure/CategoryFeature#">CategoryFeature</ulink> </para></section></section></article>